{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e6b7e4f-37ac-42c8-9545-0d552201da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TD Prediction Result ===\n",
      "  0.01   0.07   0.19   0.00 \n",
      " -0.04   ###   -0.50  -0.24 \n",
      " -0.10  -0.23  -0.44  -0.77 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==========================================\n",
    "# 1. GridWorld 환경 (step, reset 함수 추가됨)\n",
    "# ==========================================\n",
    "class gridworld:\n",
    "    def __init__(self):\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.action_meaning = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "        self.reward_map = np.array([[0, 0, 0, 1.0],\n",
    "                                    [0, None, 0, -1.0],\n",
    "                                    [0, 0, 0, 0]])\n",
    "        self.goal_state = (0, 3)\n",
    "        self.wall_state = (1, 1)\n",
    "        self.start_state = (2, 0)\n",
    "        self.agent_state = self.start_state\n",
    "\n",
    "    def height(self): return len(self.reward_map)\n",
    "    def width(self): return len(self.reward_map[0])\n",
    "    def actions(self): return self.action_space\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_state = self.start_state\n",
    "        return self.agent_state\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        state = self.agent_state\n",
    "        next_state = self.next_state(state, action)\n",
    "        reward = self.reward(state, action, next_state)\n",
    "        done = (next_state == self.goal_state)\n",
    "        \n",
    "        self.agent_state = next_state\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "        action_move_map = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        move = action_move_map[action]\n",
    "        next_state = (state[0] + move[0], state[1] + move[1])\n",
    "        ny, nx = next_state\n",
    "        \n",
    "        if nx < 0 or nx >= self.width() or ny < 0 or ny >= self.height():\n",
    "            next_state = state\n",
    "        elif next_state == self.wall_state:\n",
    "            next_state = state\n",
    "            \n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action, next_state):\n",
    "        return self.reward_map[next_state[0], next_state[1]]\n",
    "\n",
    "    def render_v(self, V):\n",
    "        for h in range(self.height()):\n",
    "            for w in range(self.width()):\n",
    "                state = (h, w)\n",
    "                if state == self.wall_state:\n",
    "                    print(\"  ###  \", end=\"\")\n",
    "                else:\n",
    "                    print(f\"{V[state]:6.2f}\", end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "# ==========================================\n",
    "# 2. TD Agent (들여쓰기 수정됨)\n",
    "# ==========================================\n",
    "class TDagent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.01\n",
    "        self.action_size = 4\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions)\n",
    "        self.V = defaultdict(lambda: 0)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    def eval(self, state, reward, next_state, done):\n",
    "        next_V = 0 if done else self.V[next_state]\n",
    "        target = reward + self.gamma * next_V\n",
    "        self.V[state] += (target - self.V[state]) * self.alpha\n",
    "\n",
    "# ==========================================\n",
    "# 3. 실행부\n",
    "# ==========================================\n",
    "env = gridworld()\n",
    "agent = TDagent()\n",
    "\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        agent.eval(state, reward, next_state, done)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state \n",
    "\n",
    "print(\"=== TD Prediction Result ===\")\n",
    "env.render_v(agent.V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
