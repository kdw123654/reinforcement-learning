{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782c82c-6de3-4148-9155-aca8ef8c01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "class gridworld:\n",
    "    def __init__(self):\n",
    "        self.action_space=[0,1,2,3]\n",
    "        self.action_meaning={0:'up',1:'down',2:'left',3:'right'}\n",
    "        self.reward_map=np.array([[0,0,0,1.0],\n",
    "                                  [0,None,0,-1.0],\n",
    "                                  [0,0,0,0]])\n",
    "        self.goal_state=(0,3)\n",
    "        self.wall_state=(1,1)\n",
    "        self.start_state=(2,0)\n",
    "        self.agent_state=self.start_state\n",
    "\n",
    "    def height(self):\n",
    "        return len(self.reward_map)\n",
    "\n",
    "    def width(self):\n",
    "        return len(self.reward_map[0])\n",
    "\n",
    "    def shape(self):\n",
    "        return self.reward_map.shape\n",
    "\n",
    "    def actions(self):\n",
    "        return self.action_space\n",
    "\n",
    "    def states(self):\n",
    "        for h in range(self.height()):\n",
    "            for w in range(self.width()):\n",
    "                yield(h,w)\n",
    "\n",
    "    def next_state(self,state,action):\n",
    "        action_move_map=[(-1,0),(1,0),(0,-1),(0,1)]\n",
    "        move=action_move_map[action]\n",
    "        next_state=(state[0]+move[0],state[1]+move[1])\n",
    "        ny,nx=next_state\n",
    "\n",
    "        if nx<0 or nx>=self.width() or ny<0 or ny>=self.height():\n",
    "            next_state=state\n",
    "        elif next_state==self.wall_state:\n",
    "            next_state=state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self,state,actin,next_state):\n",
    "        return self.reward_map[next_state[0], next_state[1]]\n",
    "\n",
    "    def render_v(self, V):\n",
    "        for h in range(self.height()):\n",
    "            for w in range(self.width()):\n",
    "                state = (h, w)\n",
    "                if state == self.wall_state:\n",
    "                    print(\"  ###  \", end=\"\")\n",
    "                else:\n",
    "                    print(f\"{V[state]:6.2f}\", end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "env=gridworld()\n",
    "\n",
    "print(env.height())\t\n",
    "print(env.width())\n",
    "print(env.shape())\n",
    "\n",
    "for action in env.actions():\n",
    "    print(action)\n",
    "print('===')\n",
    "\n",
    "for state in env.states():\n",
    "    print(state)\n",
    "\n",
    "env=gridworld()\n",
    "V={}\n",
    "for state in env.states():\n",
    "    V[state]=np.random.randn()\n",
    "\n",
    "env.render_v(V)\n",
    "\n",
    "\n",
    "env=gridworld()\n",
    "V={}\n",
    "\n",
    "for state in env.states():\n",
    "    V[state]=0\n",
    "\n",
    "state=(1,2)\n",
    "print(V[state])\n",
    "\n",
    "from collections import defaultdict\n",
    "pi=defaultdict(lambda:{0:0.25,1:0.25,2:0.25,3:0.25})\n",
    "V=defaultdict(lambda:0)\n",
    "state=(0,1)\n",
    "print(pi[state])\n",
    "\n",
    "def argmax(d):\n",
    "    return max(d, key=d.get)\n",
    "    \n",
    "def eval_onestep(pi,V,env,gamma=0.9):\n",
    "    for state in env.states():\n",
    "        if state==env.goal_state:\n",
    "            V[state]=0\n",
    "            continue\n",
    "        if state==env.wall_state:\n",
    "            continue\n",
    "        action_probs=pi[state]\n",
    "        new_V=0\n",
    "\n",
    "        for action,action_prob in action_probs.items():\n",
    "            next_state=env.next_state(state,action)\n",
    "            r=env.reward(state,action,next_state)\n",
    "            new_V+=action_prob*(r+gamma*V[next_state])\n",
    "\n",
    "        V[state]=new_V\n",
    "    return V\n",
    "\n",
    "def policy_eval(pi,V,env,gamma,threshold=0.001):\n",
    "    while True:\n",
    "        old_V=V.copy()\n",
    "        V=eval_onestep(pi,V,env,gamma)\n",
    "\n",
    "        delta=0\n",
    "        for state in V.keys():\n",
    "            t=abs(V[state]-old_V[state])\n",
    "            if delta<t:\n",
    "                delta=t\n",
    "        if delta<threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "env=gridworld()\n",
    "gamma=0.9\n",
    "V=policy_eval(pi,V,env,gamma)\n",
    "env.render_v(V)\n",
    "\n",
    "def greedy_policy(V,env,gamma):\n",
    "    pi={}\n",
    "    for state in env.states():\n",
    "      action_values={}\n",
    "    for action in env.actions():\n",
    "      next_state=env.next_state(state,action)\n",
    "      r=env.reward(state,action,next_state)\n",
    "      value=r+gamma*V[next_state]\n",
    "      action_values[action]=value\n",
    "\n",
    "      max_action=argmax(action_values)\n",
    "      action_probs={0:0,1:0,2:0,3:0}\n",
    "      action_probs[max_action]=1.0\n",
    "      pi[state]=action_probs\n",
    "    return pi\n",
    "\n",
    "def policy_iter(env,gamma,threshold=0.001,is_render=False):\n",
    "    pi=defaultdict(lambda:{0:0.25,1:0.25,2:0.25,3:0.25})\n",
    "    V=defaultdict(lambda:0)\n",
    "\n",
    "    while True:\n",
    "        V=policy_eval(pi,V,env,gamma,threshold)\n",
    "        new_pi=greedy_policy(V,env,gamma)\n",
    "        if is_render:\n",
    "            env.render_v(V,pi)\n",
    "        if new_pi==pi:\n",
    "            break\n",
    "            pi=new_pi\n",
    "    return pi\n",
    "\n",
    "env=gridworld()\n",
    "gamma=0.9\n",
    "pi=policy_iter(env,gamma)\n",
    "\n",
    "\n",
    "def value_iter_onestep(V, env, gamma):\n",
    "    for state in env.states():      \n",
    "        if state == env.goal_state: \n",
    "            V[state] = 0\n",
    "            continue\n",
    "        \n",
    "        action_values = []\n",
    "        for action in env.actions(): \n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            value = r + gamma * V[next_state]  \n",
    "            action_values.append(value)\n",
    "            \n",
    "        V[state] = max(action_values) \n",
    "    \n",
    "    return V\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- [환경 설정] 기존과 동일합니다 ---\n",
    "class gridworld:\n",
    "    def __init__(self):\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.action_meaning = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "        self.reward_map = np.array([[0, 0, 0, 1.0],\n",
    "                                    [0, None, 0, -1.0],\n",
    "                                    [0, 0, 0, 0]])\n",
    "        self.goal_state = (0, 3)\n",
    "        self.wall_state = (1, 1)\n",
    "        self.start_state = (2, 0)\n",
    "        self.agent_state = self.start_state\n",
    "\n",
    "    def height(self): return len(self.reward_map)\n",
    "    def width(self): return len(self.reward_map[0])\n",
    "    def actions(self): return self.action_space\n",
    "    def states(self):\n",
    "        for h in range(self.height()):\n",
    "            for w in range(self.width()):\n",
    "                yield (h, w)\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "        action_move_map = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        move = action_move_map[action]\n",
    "        next_state = (state[0] + move[0], state[1] + move[1])\n",
    "        ny, nx = next_state\n",
    "        if nx < 0 or nx >= self.width() or ny < 0 or ny >= self.height():\n",
    "            next_state = state\n",
    "        elif next_state == self.wall_state:\n",
    "            next_state = state\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action, next_state):\n",
    "        return self.reward_map[next_state[0], next_state[1]]\n",
    "\n",
    "    def render_v(self, V):\n",
    "        for h in range(self.height()):\n",
    "            for w in range(self.width()):\n",
    "                state = (h, w)\n",
    "                if state == self.wall_state:\n",
    "                    print(\"  ###  \", end=\"\")\n",
    "                else:\n",
    "                    print(f\"{V[state]:6.2f}\", end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "# --- [첫 번째 이미지 코드] 1회 업데이트 함수 ---\n",
    "def value_iter_onestep(V, env, gamma):\n",
    "    for state in env.states():\n",
    "        # 목표 상태에서의 가치 함수는 항상 0\n",
    "        if state == env.goal_state:\n",
    "            V[state] = 0\n",
    "            continue\n",
    "            \n",
    "        action_values = []\n",
    "        for action in env.actions():\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            \n",
    "            # 새로운 가치 함수 계산 (Bellman Optimality Equation)\n",
    "            value = r + gamma * V[next_state]\n",
    "            action_values.append(value)\n",
    "        \n",
    "        # [핵심] 최댓값 추출 (정책 평가와 가장 다른 부분!)\n",
    "        # 확률을 곱하는 게 아니라, 가장 좋은 값 하나를 선택합니다.\n",
    "        V[state] = max(action_values)\n",
    "        \n",
    "    return V\n",
    "\n",
    "def value_iter(V, env, gamma, threshold=0.001, is_render=True):\n",
    "    while True:\n",
    "        if is_render:\n",
    "            env.render_v(V)\n",
    "            \n",
    "        old_V = V.copy() \n",
    "        V = value_iter_onestep(V, env, gamma)\n",
    "        \n",
    "      \n",
    "        delta = 0\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "        \n",
    "       \n",
    "        if delta < threshold:\n",
    "            print(\"Converged!\")\n",
    "            break\n",
    "            \n",
    "    return V\n",
    "\n",
    "# --- 실행 ---\n",
    "env = gridworld()\n",
    "gamma = 0.9\n",
    "V = defaultdict(lambda: 0)\n",
    "\n",
    "\n",
    "final_V = value_iter(V, env, gamma)\n",
    "\n",
    "env.render_v(V,pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2e4bd8b-d0a0-4a44-abe0-ac8a2de274d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Policy Iteration Result ===\n",
      "  0.81   0.90   1.00   0.00 \n",
      "  0.73   ###    0.90   1.00 \n",
      "  0.66   0.73   0.81   0.73 \n",
      "\n",
      "\n",
      "=== Value Iteration Result ===\n",
      "Value Iteration Converged!\n",
      "  0.81   0.90   1.00   0.00 \n",
      "  0.73   ###    0.90   1.00 \n",
      "  0.66   0.73   0.81   0.73 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==========================================\n",
    "# 1. GridWorld 환경 클래스\n",
    "# ==========================================\n",
    "class gridworld:\n",
    "    def __init__(self):\n",
    "        self.action_space = [0, 1, 2, 3]\n",
    "        self.action_meaning = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}\n",
    "        self.reward_map = np.array([[0, 0, 0, 1.0],\n",
    "                                    [0, None, 0, -1.0],\n",
    "                                    [0, 0, 0, 0]])\n",
    "        self.goal_state = (0, 3)\n",
    "        self.wall_state = (1, 1)\n",
    "        self.start_state = (2, 0)\n",
    "        self.agent_state = self.start_state\n",
    "\n",
    "    def height(self): return len(self.reward_map)\n",
    "    def width(self): return len(self.reward_map[0])\n",
    "    def actions(self): return self.action_space\n",
    "    def states(self):\n",
    "        for h in range(self.height()):\n",
    "            for w in range(self.width()):\n",
    "                yield (h, w)\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "        action_move_map = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        move = action_move_map[action]\n",
    "        next_state = (state[0] + move[0], state[1] + move[1])\n",
    "        ny, nx = next_state\n",
    "        \n",
    "        if nx < 0 or nx >= self.width() or ny < 0 or ny >= self.height():\n",
    "            next_state = state\n",
    "        elif next_state == self.wall_state:\n",
    "            next_state = state\n",
    "            \n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action, next_state):\n",
    "        return self.reward_map[next_state[0], next_state[1]]\n",
    "\n",
    "    def render_v(self, V):\n",
    "        for h in range(self.height()):\n",
    "            for w in range(self.width()):\n",
    "                state = (h, w)\n",
    "                if state == self.wall_state:\n",
    "                    print(\"  ###  \", end=\"\")\n",
    "                else:\n",
    "                    print(f\"{V[state]:6.2f}\", end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "\n",
    "def argmax(d):\n",
    "    return max(d, key=d.get)\n",
    "\n",
    "\n",
    "def eval_onestep(pi, V, env, gamma=0.9):\n",
    "    for state in env.states():\n",
    "        if state == env.goal_state:\n",
    "            V[state] = 0\n",
    "            continue\n",
    "        if state == env.wall_state:\n",
    "            continue\n",
    "            \n",
    "        action_probs = pi[state]\n",
    "        new_V = 0\n",
    "        for action, action_prob in action_probs.items():\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            new_V += action_prob * (r + gamma * V[next_state])\n",
    "        V[state] = new_V\n",
    "    return V\n",
    "\n",
    "def policy_eval(pi, V, env, gamma, threshold=0.001):\n",
    "    while True:\n",
    "        old_V = V.copy()\n",
    "        V = eval_onestep(pi, V, env, gamma)\n",
    "        \n",
    "        delta = 0\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def greedy_policy(V, env, gamma):\n",
    "    pi = {}\n",
    "    for state in env.states():\n",
    "        if state == env.goal_state or state == env.wall_state:\n",
    "            pi[state] = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "            continue\n",
    "            \n",
    "        action_values = {}\n",
    "        for action in env.actions():\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            value = r + gamma * V[next_state]\n",
    "            action_values[action] = value\n",
    "            \n",
    "        max_action = argmax(action_values)\n",
    "        action_probs = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "        action_probs[max_action] = 1.0\n",
    "        pi[state] = action_probs\n",
    "    return pi\n",
    "\n",
    "def policy_iter(env, gamma, threshold=0.001):\n",
    "    pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})\n",
    "    V = defaultdict(lambda: 0)\n",
    "\n",
    "    while True:\n",
    "        V = policy_eval(pi, V, env, gamma, threshold)\n",
    "        new_pi = greedy_policy(V, env, gamma)\n",
    "        \n",
    "        if new_pi == pi:\n",
    "            break\n",
    "        pi = new_pi\n",
    "    return V, pi  \n",
    "\n",
    "\n",
    "def value_iter_onestep(V, env, gamma):\n",
    "    for state in env.states():\n",
    "        if state == env.goal_state:\n",
    "            V[state] = 0\n",
    "            continue\n",
    "        if state == env.wall_state: \n",
    "            continue\n",
    "            \n",
    "        action_values = []\n",
    "        for action in env.actions():\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            value = r + gamma * V[next_state]\n",
    "            action_values.append(value)\n",
    "            \n",
    "        V[state] = max(action_values) \n",
    "    return V\n",
    "\n",
    "def value_iter(V, env, gamma, threshold=0.001):\n",
    "    while True:\n",
    "        old_V = V.copy()\n",
    "        V = value_iter_onestep(V, env, gamma)\n",
    "        \n",
    "        delta = 0\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "        \n",
    "        if delta < threshold:\n",
    "            print(\"Value Iteration Converged!\")\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# ==========================================\n",
    "# 5. 실행부 (Main)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    env = gridworld()\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 1. 정책 반복법 실행\n",
    "    print(\"=== Policy Iteration Result ===\")\n",
    "    V_pi, pi = policy_iter(env, gamma)\n",
    "    env.render_v(V_pi)\n",
    "\n",
    "    # 2. 가치 반복법 실행\n",
    "    print(\"\\n=== Value Iteration Result ===\")\n",
    "    V_vi = defaultdict(lambda: 0)\n",
    "    final_V = value_iter(V_vi, env, gamma)\n",
    "    env.render_v(final_V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
